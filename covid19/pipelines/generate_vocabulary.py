import pandas as pd
from covid19.lib.objects.researchpaper import ResearchPaper
from covid19.lib.objects.buffer import Buffer
from covid19.DataManagement.loaddata import LoadData
from covid19.DataManagement.datapreprocessing import DataPreProcessing

from multiprocessing import current_process, Queue, Lock, Process
from queue import Empty
from time import time
from covid19 import logger, results_data_path, vocabulary_dir
from os import path


def sentences_producer(queue, data):
    """
    puts the list of sentences or a single sentence in the queue generated by the research paper
    :param queue: queue object
    :param data: data generator object
    :return:
    """
    count = 0
    start_time = time()
    p = current_process()
    logger.info('Running process: {} with pid: {}'.format(p.name, p.pid))
    for datum in data.get_datum():
        count += 1
        if count % 1000 == 0:
            logger.info('Approx: {} files have been processed so far'.format(count))
        rp = ResearchPaper(r_paper=datum)
        sentences = rp.get_all_sentences()
        if sentences is not None and len(sentences) > 0:
            queue.put(sentences)
    end_time = time()
    logger.info('Exiting process {} with pid: {}'.format(p.name, p.pid))
    logger.info('Process: {} with pid: {} ran for {} seconds'.format(p.name, p.pid, end_time - start_time))


def corpus_to_words_producer(corp_queue, word_queue):
    in_queue = corp_queue
    out_queue = word_queue
    p = current_process()
    start_time = time()
    logger.info('Running process: {} with pid: {}'.format(p.name, p.pid))
    try:
        while True:
            corpus = in_queue.get(timeout=60)
            unique_words = DataPreProcessing.unique_words(document=corpus)
            filtered_words = DataPreProcessing.remove_hyperlinks(unique_words)
            processed_words = DataPreProcessing.remove_special_chars(filtered_words)
            filtered_words = DataPreProcessing.remove_numbers(processed_words)
            filtered_words = DataPreProcessing.remove_by_length(filtered_words, length=2)
            filtered_words = DataPreProcessing.remove_stop_words(words=filtered_words)
            stemmed_words = DataPreProcessing.stemmer(filtered_words)

            out_queue.put(stemmed_words)
            # logger.info('Placed words of size: {} in words queue'.format(len(words)))
    except Empty:
        logger.info('For Process: {} with pid: {} No data found in the corpus queue for the last 60 seconds, '
                    'preparing to terminate'.format(p.name, p.pid))
    end_time = time()
    logger.info('Process: {} with pid: {} ran for {} seconds'.format(p.name, p.pid, end_time - start_time))


def generate_interim_vocabulary(word_queue, result_queue):
    in_queue = word_queue
    buffer = Buffer(max_size=1000)
    p = current_process()
    start_time = time()
    logger.info('Running process: {} with pid: {}'.format(p.name, p.pid))
    try:
        while True:
            words = in_queue.get(timeout=100)
            if words is None or len(words) == 0:
                continue
            else:
                words_set = set(words)
            try:
                buffer.add(words_set)
            except OverflowError:
                tmp_array = list()
                for item in buffer.buffer_data:
                    tmp_array.extend(item)

                buffer.clear()
                results = list(set(tmp_array))
                logger.info('Writing {} results to results queue for Process: {} with pid: {}'.format(len(results),
                                                                                                      p.name, p.pid))
                result_queue.put(results)
    # this block of code is executed if in_queue is empty for 10 seconds
    except Empty:
        tmp_array = list()
        if buffer.size() > 0:
            for item in buffer.buffer_data:
                tmp_array.extend(item)

            buffer.clear()
            logger.info("No data found in the words_queue for last 100 seconds")
            results = list(set(tmp_array))
            logger.info("Emptying left over results in the buffer of size {}".format(len(results)))
            result_queue.put(results)
    end_time = time()
    logger.info('Process: {} with pid: {} ran for {} seconds'.format(p.name, p.pid, end_time - start_time))


if __name__ == "__main__":
    corpus_queue = Queue(maxsize=100)
    words_queue = Queue()
    results_queue = Queue(maxsize=100)
    lock = Lock()

    data_gen_process = Process(target=sentences_producer, args=(corpus_queue, LoadData(), ), name='sentence producer')

    corpus_to_words_process = [Process(target=corpus_to_words_producer, args=(corpus_queue, words_queue, ),
                                       name='corpus to word process') for i in range(10)]
    generate_vocabulary_process = [Process(target=generate_interim_vocabulary, args=(words_queue, results_queue, ),
                                           name= 'generate_interim_vocabulary process') for i in range(10)]

    # Starting all Process
    # starting data generation process
    logger.info('Starting {} process having pid {}'.format(data_gen_process.name, data_gen_process.pid))
    data_gen_process.start()
    # starting corpus to word process
    for process in corpus_to_words_process:
        process.daemon = True
        logger.info('Starting {} process having pid {}'.format(process.name, process.pid))
        process.start()

    # starting generate vocabulary process
    for process in generate_vocabulary_process:
        process.daemon = True
        logger.info('Starting {} process having pid {}'.format(process.name, process.pid))
        process.start()

    corpus_vocabulary = set()
    while any([process.is_alive() for process in corpus_to_words_process]) or results_queue.empty() is False:
        try:
            vocab_to_add = set(results_queue.get(timeout=300))
            corpus_vocabulary.update(vocab_to_add)
            logger.info('updating vocabulary of size: {}'.format(len(vocab_to_add)))
            logger.info(len(corpus_vocabulary))

            # update the data frame with the results generated and write to file

        except Empty:
            logger.info("No results found in the  result_queue for the past 300 seconds")
            logger.info('Here is the status of the corpus to words process')
            for process in corpus_to_words_process:
                logger.info("p_id: {}, pid: {}, is_alive:{}".format(process.name, process.pid, process.is_alive()))

    # wait till child process complete
    data_gen_process.join()
    for process in corpus_to_words_process:
        process.join()

    for process in generate_vocabulary_process:
        process.join()

    results_file = path.join(vocabulary_dir, 'global_vocabulary.csv')

    logger.info('Generating global data frame')
    voc_df = pd.DataFrame({'vocab': list(corpus_vocabulary)})
    logger.info('Total length of vocabulary is {}'.format(len(corpus_vocabulary)))
    logger.info('Writing vocabulary to "{}"'.format(results_file))
    voc_df.to_csv(results_file)
