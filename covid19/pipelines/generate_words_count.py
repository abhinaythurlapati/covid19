import pandas as pd
from covid19.lib.objects.researchpaper import ResearchPaper
from covid19.lib.objects.buffer import Buffer
from covid19.DataManagement.loaddata import LoadData
from covid19.DataManagement.datapreprocessing import DataPreProcessing

from multiprocessing import current_process, Queue, Lock, Process
from queue import Empty
from time import time
from covid19 import logger
from covid19 import results_data_path
from os import path


def sentences_producer(queue, data):
    """
    puts the list of sentences or a single sentence in the queue generated by the research paper
    :param queue: queue object
    :param data: data generator object
    :return:
    """
    count = 0
    start_time = time()
    p = current_process()
    logger.info('Running process: {} with pid: {}'.format(p.name, p.pid))
    for datum in data.get_datum():
        count += 1
        if count % 1000 == 0:
            logger.info('Approx: {} files have been processed so far'.format(count))
        rp = ResearchPaper(r_paper=datum)
        sentences = rp.get_all_sentences()
        if sentences is not None and len(sentences) > 0:
            queue.put(sentences)
    end_time = time()
    logger.info('Exiting process {} with pid: {}'.format(p.name, p.pid))
    logger.info('Process: {} with pid: {} ran for {} seconds'.format(p.name, p.pid, end_time - start_time))


def corpus_to_words_count_producer(corp_queue, word_queue):
    in_queue = corp_queue
    out_queue = word_queue
    p = current_process()
    start_time = time()
    logger.info('Running process: {} with pid: {}'.format(p.name, p.pid))
    try:
        while True:
            corpus = in_queue.get(timeout=60)
            words_count = DataPreProcessing.word_counts(document=corpus, remove_hyperlinks=True,
                                                        remove_special_chars=True, remove_stop_words=True)

            out_queue.put(words_count)
            logger.info('Placed words count dict of size: {} in words queue'.format(len(words_count)))
    except Empty:
        logger.info('For Process: {} with pid: {} No data found in the corpus queue for the last 60 seconds, '
                    'preparing to terminate'.format(p.name, p.pid))
    end_time = time()
    logger.info('Process: {} with pid: {} ran for {} seconds'.format(p.name, p.pid, end_time - start_time))


if __name__ == "__main__":
    corpus_queue = Queue(maxsize=100)
    words_count_queue = Queue()
    global_word_count_dict = dict()

    data_gen_process = Process(target=sentences_producer, args=(corpus_queue, LoadData(), ), name='sentence producer')

    corpus_to_words_count_process = [Process(target=corpus_to_words_count_producer, args=(corpus_queue, words_count_queue, ),
                                       name='corpus to words count process') for i in range(10)]

    # Starting all Process
    # starting data generation process
    logger.info('Starting {} process having pid {}'.format(data_gen_process.name, data_gen_process.pid))
    data_gen_process.start()

    # starting corpus to words count process
    for process in corpus_to_words_count_process:
        process.daemon = True
        logger.info('Starting {} process having pid {}'.format(process.name, process.pid))
        process.start()

    corpus_vocabulary = set()

    while any([process.is_alive() for process in corpus_to_words_count_process]) or words_count_queue.empty() is False:
        try:
            words_count_dict = words_count_queue.get(timeout=300)
            for word, word_count in words_count_dict.items():
                if word not in global_word_count_dict:
                    global_word_count_dict[word] = word_count
                else:
                    global_word_count_dict[word] += word_count

        except Empty:
            logger.info("No results found in the  result_queue for the past 300 seconds")
            logger.info('Here is the status of the corpus to words process')
            for process in corpus_to_words_count_process:
                logger.info("p_id: {}, pid: {}, is_alive:{}".format(process.name, process.pid, process.is_alive()))

    # wait till child process complete
    data_gen_process.join()
    for process in corpus_to_words_count_process:
        process.join()

    logger.info('Generating words count data frame')

    words_count_df = pd.DataFrame(list(global_word_count_dict.items()), columns=['word', 'count'])
    logger.info('Total length of vocabulary is {}'.format(len(global_word_count_dict)))
    logger.info('Writing words_count dataframe to "global_words_count.csv"')
    words_count_df.to_csv(path.join(results_data_path, 'global_title_vocabulary.csv'))
